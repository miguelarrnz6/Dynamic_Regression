---
title: "Dynamic Model 2: Model Building"
author: "Miguel A. Arranz"
date: "November 2016"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goals
Build a dynamic linear regression model


# Model Building

## Strategies

+ Specific to General

+ General to Specific

## Testing

+ Coefficients

+ Residuals

+ Model

### Consistent Inference

We need to obtain HAC covariance matrix estimators. The usual covariance matrix estimators are not consistent if there is serial correlation or heteroskedasticity (very common when dealing with time series), leading to incorrect inference.

When performing significance tests of the coefficient (individual significance, redundant variables, Granger non-causality) it is crucial to apply a HAC covariance matrix estimator.

### Normality

**Jarque-Bera test**

The null hypothesis is that they follow a Normal distribution

### Serial Correlation

The model is

$$ y_t = \delta_0 + \sum_{i=1}^p \delta_i y_{t-i} + \sum_{i=0}^q \beta_i x_{t-i} + \epsilon_i$$

+ **Durbin**

After estimating the regression by OLS we obtain the residuals and regress those residuals on the original regressors of the equation and lagged values of the residuals (auxiliary regression).

$$ \hat{\epsilon}_t = \sum_{i=1}^p \delta_i y_{t-i} + \sum_{i=0}^q \beta_i x_{t-i} + 
 \gamma_1 \hat{\epsilon}_{t-1} + \ldots + \gamma_k \hat{\epsilon}_{t-k} + \nu_t$$
and test
$$ H_0: \gamma_1 = \gamma_2 = \ldots = \gamma_k = 0$$
using a Wald test (with a HAC covariance matrix estimator)

 + **Breusch-Godfrey test**
 
 We make use of the residuals of the original regression and build an auxiliary regression:
 
 $$ \hat{\epsilon}_t = \sum_{i=1}^p \delta_i y_{t-i} + \sum_{i=0}^q \beta_i x_{t-i} + 
 \gamma_1 \hat{\epsilon}_{t-1} + \ldots + \gamma_k \hat{\epsilon}_{t-k} + \nu_t$$
 and test
$$ H_0: \gamma_1 = \gamma_2 = \ldots = \gamma_k = 0$$We want to test the null hypothesis
 The test statistic we use is the product of $T$, the number of observations used in the auxiliary regression, and the $R^2$ of the auxiliary regression. Under H_0:
 $$ T R^2\sim \chi^2_k$$
 and if we fail to reject $H_0$ we cannot reject that there is no correlation up to lag $k$.
 
 
 **Note:** There are small sample corrections to both test that we are not applying.
 
 + **Ljung-Box test**
 
 This is the same test we have used in the past. 

### Heteroskedasticity

+ **White test**

White's test is a test of the null hypothesis of no heteroskedasticity against keteroskedasticity of unknown, general form.

Auxiliary regression where the dependent variable is the squared residuals and the dependent variables are all possible (non-redundant) cross products of the original regressors. For example, if the original regression is
$$ y_t = \beta_0 + \beta_1 x_t + \beta_2 z_t + \epsilon_t$$
the auxiliary regression is
$$ \hat{\epsilon}_t^2 = \alpha_0 + \alpha_1 x_t + \alpha_2 z_t +
\alpha_3 x_t^2 + \alpha_4 z_t^2 + \alpha_5 x_t z_t + \nu_t$$

The null hypothesis is
$$ H_0: \alpha_1 = \alpha_2 = \ldots = \alpha_5 = 0$$
The usual statistic is $T R^2$, and it follows under $H_0$ a $\chi^2$ distribution with degrees of freedom equal to the number of coefficients of the auxiliary regression excluding the intercept.

Notice that we might end up using too many variables in the auxiliary model if the original model has many variables. We need some strategies to eliminate variables in the auxiliary model.

We can eliminate the linear effects (keeping interactions and quadratic effects) if the original regression has no intercept.

We can also eliminate cross products, keeping linear and quadratic effects.

This a very generic test, lacking power when we include too many parameters in the auxiliary regression. It might also detect some other misspecifications of the model.


+ **Breusch-Pagan test**

In this case we are more specific about the functional form of the variance:

$$ E(\epsilon_t^2) = f(z_t)$$
where $z$ are the variables in the original regression.
In the BP test the auxiliary regression is

$$\hat{\epsilon}_t^2 = \gamma_0 + \gamma_1 z_{1,t} + \ldots + \gamma_k z_{k,t} + \nu_t$$
Remember that $z$ are the variables included in the original regression (lags of $y$ and $x$).
The test statistic under $H_0$ is
$$ TR^2 \sim \chi^2(k)$$


Another version of the test is the one proposed by **Harvey**

The only difference is that the auxiliary regression is
$$\log(\hat{\epsilon}_t^2) = \gamma_0 + \gamma_1 z_{1,t} + \ldots + \gamma_k z_{k,t} + \nu_t$$
There are some small sample corrections that we are not considering.

### ARCH effects

**Conditional heteroskedasticity**

$$ E(\epsilon^2_t | I_{t-1}) = \alpha_0 + \alpha_1 \epsilon^2_{t-1} + \ldots + \alpha_p \epsilon^2_p$$

+ Visual inspection

Graph the ACF of the squared residuals of the original equation

+ **ARCH-LM Test**

The test is based on the squared residuals of the original equation. We form an auxiliary regression
 $$ \hat{\epsilon}^2_t = \theta_0 + \theta_1 \hat{\epsilon}^2_{t-1} + \ldots + \theta_k \hat{\epsilon}^2_{t-k} + \nu_t$$
We want to test the null hypothesis
 $$ H_0: \quad \theta_1 = \ldots = \theta_k = 0$$
 and test statistic we use is the product of $T$, the number of observations used in the auxiliary regression, and the $R^2$ of the auxiliary regression. Under H_0:
 $$ T R^2\sim \chi^2_k$$
 and if we fail to reject $H_0$ we cannot reject that there is no ARCH up to lag $k$.

Two points:

+ You can never remove the constant from the auxiliary regression.

+ This test has rather low power. One of the reasons is including too many lags in the auxiliary regression. If you reject $H_0$ with a low number of lags you can be certain that there is ARCH


### Roots of polynomials

Write the model as:
$$ \delta(L) y_t = \delta_0 + \beta(L) x_t + \epsilon_t$$
or as
$$ y_t = \dfrac{\delta_0}{\delta(L)} + 
\dfrac{\beta(L)}{\delta(L)} x_t + \dfrac{1}{\delta(L)} \epsilon_t $$
+ Causality

This refers to the absolute values of the roots of the AR polynomial being less than 1.

+ Common roots

Check in case you can make the calculations of the dynamic effects of shocks on $x$ easier.

### Granger (Non-)Causality

It is a Wald-type test:

$$ y_t = \delta_0 + \sum_{i=1}^p \delta_i y_{t-i} + \sum_{i=1}^q \beta_i x_{t-1} + \epsilon_t $$
Null hypothesis
$$ H_0: \quad \beta_1 = \beta_2 = \ldots = \beta_q = 0 $$
Restricted model
$$ y_t = \delta_0 + \sum_{i=1}^p \delta_i y_{t-i} + \epsilon_t$$
If we cannot reject $H_0$ then $x$ **does not cause** $y$.
# Data Analysis

# Packages and Data

```{r packages, message=FALSE, warning=FALSE}
library(quantmod)
library(dynlm)
library(tseries)
library(forecast)
library(lmtest)
library(sandwich)
library(ggplot2)
library(plotly)
library(tsoutliers)
source("myfuncs.R")
```

```{r datadownload, message=FALSE, warning=FALSE}
symbols.vec <- c("INDPRO", "GS10")

getSymbols(symbols.vec, src="FRED")

INDPRO <- INDPRO['2000-01-01/2015-12-31']
GS10 <- GS10['2000-01-01/2015-12-31']

y <- as.zoo(diff(log(INDPRO)))
x <- as.zoo(diff(GS10))

```



# Specific to General

When we build an ARIMA model we got an AR(4) specification without intercept. Hence, we start with an AR(4) model with intercept (just to test again) and add the contemporary and one lagged values of $X$.

$$ y_t = \delta_0 + \sum_{i=1}^4 \delta_i y_{t-i} + \beta_0 x_t + \beta_1 x_{t-1} + \epsilon_t $$

```{r m41}
m41 <- dynlm(y ~ L(y) + L(y,2) + L(y,3) + L(y,4) + x + L(x))
print(summary(m41))
tsdisplay(m41$resid)
coeftest(m41)
coeftest(m41, vcovHAC)
```


We drop $y_{t-1}$

```{r m41a}
m41a <- dynlm(y ~ L(y,2) + L(y,3) + L(y,4) + x + L(x))
print(summary(m41a))
tsdisplay(m41a$resid)
coeftest(m41a)
coeftest(m41a, vcovHAC)
```


Add $y_{t-1}$ and drop $x_t$

```{r m41b}
m41b <- dynlm(y ~ L(y) + L(y,2) + L(y,3) + L(y,4)  + L(x))
print(summary(m41b))
tsdisplay(m41b$resid)
coeftest(m41b)
coeftest(m41b, vcovHAC)
```


Drop $y_{t-1}$

```{r m41c}
m41c <- dynlm(y ~ L(y,2) + L(y,3) + L(y,4)  + L(x))
print(summary(m41c))
tsdisplay(m41c$resid)
coeftest(m41c)
coeftest(m41c, vcovHAC)
```

```{r m41c2}
phpoly <- c(1, 0, -coef(m41c)[2:4])
rot <- polyroot(phpoly)
print(abs(rot))
```

Drop the intercept.
Our final model is

$$y_t = \delta_2 y_{t-2} + \delta_3 y_{t-3} + \delta_4 y_{t-4} + \beta_1 x_{t-1} + \epsilon_t $$


```{r m41d}
m41d <- dynlm(y ~ -1 + L(y,2) + L(y,3) + L(y,4)  + L(x,1))
print(summary(m41d))
tsdisplay(m41d$resid)
coeftest(m41d)
coeftest(m41d, vcovHAC)

```

```{r m41d2}
phpoly <- c(1, 0, -coef(m41d)[1:3])
rot <- polyroot(phpoly)
print(abs(rot))

```


We include some variables just to check.

```{r m41e}
m41e <- dynlm(y ~ -1 + L(y,2) + L(y,3) + L(y,4)  + L(x,1) + L(x,2) + L(x,3))
print(summary(m41e))
tsdisplay(m41e$resid)
coeftest(m41e)
coeftest(m41e, vcovHAC)

```

One alternative model

```{r m43}
m43 <- dynlm(y ~ -1 + L(y,2) + L(y,3) + L(y,4)  + L(x,1) + L(x,3))
print(summary(m43))
tsdisplay(m43$resid)
coeftest(m43)
coeftest(m43, vcovHAC)
```


# General to Specific

We start with a very general model

$$ y_t = \delta_0 + \sum_{i=1}^5 \delta_i y_{t-i} + \beta_0 x_t + \sum_{i=1}^5 \beta_i x_{t-i} + \epsilon_t $$

```{r mgeneral}
mgeneral <- dynlm( y ~ L(y, 1:5) + x + L(x, 1:5))
print(summary(mgeneral))
tsdisplay(mgeneral$resid)
coeftest(mgeneral, vcov=vcovHAC)
```

```{r mbig1}
mbig <- dynlm( y ~ L(y,2) + L(y,3) + L(y,4) + x + L(x,1) + L(x,3))
print(summary(mbig))
```

```{r mbig2}
mbig <- dynlm( y ~ L(y,2) + L(y,3) + L(y,4)  + L(x,1) + L(x,3))
print(summary(mbig))
```

We drop the intercept and get our final model
$$y_t = \delta_2 y_{t-2} + \delta_3 y_{t-3} + \delta_4 y_{t-4} + \beta_1 x_{t-1} + \beta_3 x_{t-3} + \epsilon_t $$

```{r mbig3}
mbig <- dynlm( y ~ -1 + L(y,2) + L(y,3) + L(y,4)  + L(x,1) + L(x,3))
print(summary(mbig))
coeftest(mbig, vcov=vcovHAC)
tsdisplay(mbig$resid)

```

Notice that we have obtained a different model: $x_{t-3}$ has been included in our model

### Actual and Fitted Data

Plot the actual and fitted data.
Notice that actual values and fitted data start at different dates, because we have lost some observations due to the dynamics of the model. We solve the problem by merging the data and keeping the common dates.
(we have done this before with ggplot2 and plotly)

```{r fitact}
uhat <- mbig$residuals
yhat <- mbig$fitted.values
fdata <- merge(yhat, y, all = F)
names(fdata) <- c("Fitted", "Actual")
autoplot(fdata, facets = NULL) + labs(title="Dynamic Linear Model",
                                     subtitle="Actual and Fitted Data")
p <- autoplot(fdata, facets = NULL) + labs(title="Dynamic Linear Model",
                                           subtitle="Actual and Fitted Data")
ggplotly(p)
```


### Total cumulative effect

The effect is equal to
$$ \dfrac{\beta(1)}{\delta(1)} = \dfrac{\sum_{i=0}^q \beta_i}{1 - \sum_{i=1}^p \delta_i}$$ 

```{r totaleff}
betas <- mbig$coef
num <- sum(betas[4:5])
den <- 1 - sum(betas[1:3])
lreffect <- num/den
cat("Total cumulative effect ", lreffect, "\n")
```


# Testing

## Polynomials

**Causality**

Check the roots of the AR polynomial

```{r mbigcoeff}
betas <- mbig$coefficients
betas
```

```{r caus}
arpoly <- c(1,0, -betas[1:3] )
rot <- polyroot(arpoly)
rot
print(abs(rot))
```

**Common Roots**

```{r comro}
mapoly <- c(0,betas[4], 0, betas[5])
rot2 <- polyroot(mapoly)
rot2
```

## Normality

```{r jb}
JarqueBera.test(mbig$residuals)
```

In this case we **reject** Normality. It is the effect of **outliers**.

## Serial Correlation

###Breusch-Godfrey

```{r bgtest}
bgtest(mbig, 4, type="Chisq")
bgtest(mbig, 6, type="Chisq")
```

###Ljung-Box

```{r lbq}
uhat <- mbig$residuals
Box.test(coredata(uhat), lag=6, type="Ljung-Box", fitdf=length(mbig$coefficients))
Box.test(coredata(uhat), lag=12, type="Ljung-Box", fitdf=length(mbig$coefficients))
```



## Heterokedasticity


###White

```{r white1}
uhat2 <- uhat*uhat
aux <- dynlm(uhat2 ~ ( L(y,2) + L(y,3) + L(y,4)  + L(x,1) + L(x,3))*
               ( L(y,2) + L(y,3) + L(y,4)  + L(x,1) + L(x,3))
             +I(L(y,2)^2)+ I(L(y,3)^2) + I(L(y,4)^2) + I(L(x,1)^2) + I(L(x,3)^2))
summary(aux)
saux <- summary(aux)
tr2 <- length(saux$resid)*saux$r.squared
cat("White Test ", tr2, "p-Value ", pchisq(tr2, (length(aux$coefficients)-1), lower.tail = F))
```


```{r white2}
aux <- dynlm(uhat2 ~  L(y,2) + L(y,3) + L(y,4)  + L(x,1) + L(x,3)
             +I(L(y,2)^2)+ I(L(y,3)^2) + I(L(y,4)^2) + I(L(x,1)^2) + I(L(x,3)^2))
summary(aux)
saux <- summary(aux)
tr2 <- length(saux$resid)*saux$r.squared
cat("White Test ", tr2, "p-Value ", pchisq(tr2, (length(aux$coefficients)-1), lower.tail = F))
```

Drop linear terms because there is not constant in the original equation

```{r white3}
aux <- dynlm(uhat2 ~ ( L(y,2) + L(y,3) + L(y,4)  + L(x,1) + L(x,3))*
               ( L(y,2) + L(y,3) + L(y,4)  + L(x,1) + L(x,3))
             +I(L(y,2)^2)+ I(L(y,3)^2) + I(L(y,4)^2) + I(L(x,1)^2) + I(L(x,3)^2)
              - L(y,2) - L(y,3) - L(y,4)  - L(x,1) - L(x,3))
summary(aux)
saux <- summary(aux)
tr2 <- length(saux$resid)*saux$r.squared
cat("White Test ", tr2, "p-Value ", pchisq(tr2, (length(aux$coefficients)-1), lower.tail = F))

```



###Breusch-Pagan Test

```{r bptest1}
bptest(mbig)
```

```{r bptest2}
bptest(mbig, studentize = F)
```

```{r mybptest}
uhat2 <- uhat*uhat
aux <- dynlm(uhat2 ~ L(y,2) + L(y,3) + L(y,4)  + L(x,1) + L(x,3))
saux <- summary(aux)
print(saux)
tr2 <- length(saux$resid)*saux$r.squared
cat("Breusch-Pagan ", tr2, "p-Value ", pchisq(tr2, (length(aux$coefficients)-1), lower.tail = F))
```

Other version

```{r mbptest2}
aux <- dynlm(uhat2 ~ yhat)
saux <- summary(aux)
print(saux)
tr2 <- length(saux$resid)*saux$r.squared
cat("Breusch-Pagan2 ", tr2, "p-Value ", pchisq(tr2, 1, lower.tail = F))

```


###Harvey

```{r harvey}
aux <- dynlm(I(log(uhat2) ~ L(y,2) + L(y,3) + L(y,4)  + L(x,1) + L(x,3)))
saux <- summary(aux)
print(saux)
tr2 <- length(saux$resid)*saux$r.squared
cat("Harvey Test ", tr2, "p-Value ", pchisq(tr2, (length(aux$coefficients)-1), lower.tail = F))
```


## ARCH

### Graphical Analysis

```{r u2acf}
Acf(as.xts(uhat2), main="Squared Residuals")
p <- Acf(as.xts(uhat2), main="Squared Residuals")
autoplot(p) + ggtitle("Squared Residuals")
```

```{r u2acf2}
autoplot(p) + labs(
  title = "Squared Residuals",
  subtitle = "ACF",
  caption = "Data from FRED"
)

```

###ARCH-LM

```{r archlm4}
aux <- dynlm(uhat2 ~ L(uhat2, 1:4))
saux <- summary(aux)
print(saux)
tr2 <- length(saux$resid)*saux$r.squared
cat("ARCH(4) Test ", tr2, "p-Value ", pchisq(tr2, 4, lower.tail = F))
```


```{r archlm1}
aux <- dynlm(uhat2 ~ L(uhat2, 1))
saux <- summary(aux)
print(saux)
tr2 <- length(saux$resid)*saux$r.squared
cat("ARCH(1) Test ", tr2, "p-Value ", pchisq(tr2, 1, lower.tail = F))
```

**WE HAVE A PROBLEM HERE**



## Granger (Non-)Causality

```{r msmall}
msmall <- dynlm( y ~ -1 + L(y,2) + L(y,3) + L(y,4))
```

```{r waldgranger}
waldtest(msmall, mbig, vcov=vcovHAC)
waldtest(msmall, mbig, test="Chisq", vcov=vcovHAC)
```

I wrote a little function

```{r myfunc}
sgeqgcausality <- function(vY, vX, ylags, xlags){
  #Function to perform Granger causality test 
  # in single equation
  # vY and vX are the variables
  # xlags and ylags are the number of lags
  require("sandwich")
  p <- ylags + 1
  q <- xlags + 1
  z <- max(c(p,q))
  
  mY <- embed(as.vector(vY),z)
  mX <- embed(as.vector(vX),z)
  
  
  aux1 <- lm(mY[,1] ~ mY[,2:p] + mX[,2:q])
  aux2 <- lm(mY[,1] ~ mY[,2:p])
  
  waldtest(aux1, aux2, test="Chisq", vcov=vcovHAC)
  
}
```


and we can use it

```{r usefunc}
sgeqgcausality(y, x, 4, 3)
```
The result is different because the models are different:

+ I include an intercept in both models

+ I do not allow for zeros


## Compare the models

We have obtained two models

```{r cmodels, error=FALSE, message=TRUE, warning=FALSE, include=FALSE}
library(memisc)
memisc::mtable(m41d, mbig)
library(stargazer)
stargazer(m41d, mbig, type="text", star.char="", style="qje")
```


Using HAC covariance estimator

```{r haccomp, message=FALSE, warning=FALSE}
library(stargazer)
c1 <- coeftest(m41d, vcov=vcovHAC)
c2 <- coeftest(mbig, vcov= vcovHAC)
stargazer(c1, c2, type="text", style="aer", omit.table.layout = "n", star.char = "")
```


# Outliers?

ARIMAX model

```{r arimax}
ys <- diff(log(INDPRO))
xs <- diff(GS10)
xvars <- cbind(lag(xs, -1), lag(xs, -3))
mx <- Arima(ys, order=c(4,0,0), xreg = xvars, include.mean = F)
P <- armatable(mx)
print(P)
```

Outliers?

```{r}
ys <- as.ts(ys)
xs <- as.ts(xs)
```

```{r tso ar}
modout <- tso(ys, types=c("AO", "LS", "TC"),
              remove.method="bottom-up", tsmethod="auto.arima", args.tsmethod = list(max.p=5, max.q=0))
print(modout)

ZZ <- outliers.effects(modout$outliers, n=length(y))
ZZoo <- zoo(ZZ, index(y))
autoplot(ZZoo)

```

Which outliers?

```{r tso2}
index(y)[modout$times]
```



And now let's try the ARMAX model by introducing the ZZ external regressors as matrix ($ZX$)

```{r arx4}
ZX <- as.xts(ZZoo)
arx4 <- Arima(as.xts(ys), order=c(4,0,0), xreg=ZX, include.mean=F)
#print(arx4)
P <- armatable(arx4)
print(P)

```

Some tests:

```{r}
tsdisplay(arx4$resid)
Box.test(arx4$residuals, lag = 12)
JarqueBera.test(arx4$residuals)
```


Outliers are significant when we estimate the model without $X$, but they are no longer relevant when we include $X$ in the model.

```{r eval=FALSE}
modout <- tso(ys, xreg=xvars, types=c("AO", "LS", "TC"),
              remove.method="bottom-up", tsmethod="arima", args.tsmethod = list(order=c(4,0,0), xreg=xvars, include.mean=F))
print(modout)
```

But they are significant! $X_{t-1}$ is not significant
```{r}
NX <- cbind(ZX, xvars)
mx2 <- Arima(ys, order=c(4,0,0), xreg = NX, include.mean = F)
P <- armatable(mx2)
print(P)
```

Some tests:

```{r}
tsdisplay(mx2$resid)
Box.test(mx2$residuals, lag = 12)
JarqueBera.test(mx2$residuals)
```
Residuals are Normal when we take into account the outliers.


# Checking other packages

## GETS


```{r gets}
library(gets)


Xregs <- merge(lag(x, -1), lag(x,-3))
Mod3.arx <-arx(y, ar = 2:4, mxreg = Xregs)
print(Mod3.arx)
```
We can check whether parameters are constant over time using recursive estimation (more in next lesson)

```{r recursive}
recMode3.arx <- recursive(Mod3.arx)

```

## DYN

```{r dyn}

library(dyn)
mod.dyn <- dyn$lm(y ~ lag(y, -2:-4) + lag(x, -1) +lag(x,-3) - 1)
print(summary(mod.dyn))
```


## LM

```{r lm}
mydata <- cbind(y, lag(y, -2), lag(y, -3), lag(y, -4), lag(x, -1), lag(x, -3))
names(mydata) <- c("Ylag0", "Ylag2", "Ylag3", "Ylag4", "Xlag1", "Xlag3")
modlm <- lm(Ylag0 ~ Ylag2 + Ylag3 + Ylag4 + Xlag1 + Xlag3 - 1, data=mydata)
print(summary(modlm))
```

